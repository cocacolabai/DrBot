{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking using SemHash on NLU Evaluation Corpora\n",
    "\n",
    "This notebook benchmarks the results on the 3 NLU Evaluation Corpora:\n",
    "1. Ask Ubuntu Corpus\n",
    "2. Chatbot Corpus\n",
    "3. Web Application Corpus\n",
    "\n",
    "\n",
    "More information about the dataset is available here: \n",
    "\n",
    "https://github.com/sebischair/NLU-Evaluation-Corpora\n",
    "\n",
    "\n",
    "* Semantic Hashing is used as a featurizer. The idea is taken from the paper:\n",
    "\n",
    "https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/\n",
    "\n",
    "* Benchmarks are performed on the same train and test datasets used by the other benchmarks performed in the past. One important paper that benchmarks the datasets mentioned above on some important platforms (Dialogflow, Luis, Watson and RASA) is : \n",
    "\n",
    "http://workshop.colips.org/wochat/@sigdial2017/documents/SIGDIAL22.pdf\n",
    "\n",
    "* Furthermore, Botfuel made another benchmarks with more platforms (Recast, Snips and their own) and results can be found here: \n",
    "\n",
    "https://github.com/Botfuel/benchmark-nlp-2018\n",
    "\n",
    "* The blogposts about the benchmarks done in the past are available at : \n",
    "\n",
    "https://medium.com/botfuel/benchmarking-intent-classification-services-june-2018-eb8684a1e55f\n",
    "\n",
    "https://medium.com/snips-ai/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a\n",
    "\n",
    "* To be very fair on our benchmarks and results, we used the same train and test set used by the other benchmarks and no cross validation or stratified splits were used. The test data was not used in any way to improve the results. The dataset used can be found here:\n",
    "\n",
    "https://github.com/Botfuel/benchmark-nlp-2018/tree/master/results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\python36.zip', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\DLLs', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\lib', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\lib\\\\site-packages', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\gusgru\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\gusgru\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# import os\n",
    "# os.environ['LDFLAGS'] = '-framework CoreFoundation -framework SystemConfiguration'\n",
    "# !pip3 install spacy\n",
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import csv\n",
    "import spacy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn import model_selection\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "# import locale\n",
    "# print(locale.getlocale())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy english dataset with vectors needs to be present. It can be downloaded using the following command:\n",
    "\n",
    "python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# !python -m spacy download en_core_web_lg\n",
    "nlp=spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = {x.name().split('.', 1)[0] for x in wordnet.all_synsets('n')}\n",
    "verbs = {x.name().split('.', 1)[0] for x in wordnet.all_synsets('v')}\n",
    "\n",
    "def get_synonyms(word, number= 3):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonyms.append(l.name().lower().replace(\"_\", \" \"))\n",
    "    synonyms = list(OrderedDict.fromkeys(synonyms))\n",
    "    return synonyms[:number]\n",
    "    #return [token.text for token in most_similar(nlp.vocab[word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['search', 'hunt', 'hunting', 'lookup', 'seek', 'look for', 'look', 'research']\n"
     ]
    }
   ],
   "source": [
    "print(get_synonyms(\"search\",-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "benchmark_dataset = 'Chatbot' # Choose from 'AskUbuntu', 'Chatbot' or 'WebApplication'\n",
    "oversample = True             # Whether to oversample small classes or not. True in the paper\n",
    "synonym_extra_samples = False # Whether to replace words by synonyms in the oversampled samples. True in the paper\n",
    "augment_extra_samples = True  # Whether to add random spelling mistakes in the oversampled samples. False in the paper\n",
    "additional_synonyms = 0       # How many extra synonym augmented sentences to add for each sentence. 0 in the paper\n",
    "additional_augments = 0       # How many extra spelling mistake augmented sentences to add for each sentence. 0 in the paper\n",
    "mistake_distance = 2.1        # How far away on the keyboard a mistake can be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_dataset == \"Chatbot\":\n",
    "    intent_dict = {\"DepartureTime\":0, \"FindConnection\":1}\n",
    "elif benchmark_dataset == \"AskUbuntu\":\n",
    "    intent_dict = {\"Make Update\":0, \"Setup Printer\":1, \"Shutdown Computer\":2, \"Software Recommendation\":3, \"None\":4}\n",
    "elif benchmark_dataset == \"WebApplication\":\n",
    "    intent_dict = {\"Download Video\":0, \"Change Password\":1, \"None\":2, \"Export Data\":3, \"Sync Accounts\":4,\n",
    "                  \"Filter Spam\":5, \"Find Alternative\":6, \"Delete Account\":7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = \"datasets/KL/\" + benchmark_dataset + \"/train.csv\"\n",
    "filename_test = \"datasets/KL/\" + benchmark_dataset + \"/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_CSV_datafile(filename):    \n",
    "    X = []\n",
    "    y = []\n",
    "    with open(filename,'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            X.append(row[0])\n",
    "            if benchmark_dataset == 'AskUbuntu':\n",
    "                 y.append(intent_dict[row[1]])\n",
    "            elif benchmark_dataset == 'Chatbot':\n",
    "                y.append(intent_dict[row[1]])\n",
    "            else:\n",
    "                y.append(intent_dict[row[1]])           \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Returns a list of strings containing each token in `sentence`\n",
    "    \"\"\"\n",
    "    #return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\",\n",
    "    #                            doc) if i != '' and i != ' ' and i != '\\n']\n",
    "    tokens = []\n",
    "    doc = nlp.tokenizer(doc)\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    clean_tokens = []\n",
    "    doc = nlp(doc)\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            clean_tokens.append(token.lemma_)\n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********* Data augmentation part **************\n",
    "class MeraDataset():\n",
    "    \"\"\" Class to find typos based on the keyboard distribution, for QWERTY style keyboards\n",
    "    \n",
    "        It's the actual test set as defined in the paper that we comparing against.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        \"\"\" Instantiate the object.\n",
    "            @param: dataset_path The directory which contains the data set.\"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.X_test, self.y_test, self.X_train, self.y_train = self.load()\n",
    "        self.keyboard_cartesian = {'q': {'x': 0, 'y': 0}, 'w': {'x': 1, 'y': 0}, 'e': {'x': 2, 'y': 0},\n",
    "                                   'r': {'x': 3, 'y': 0}, 't': {'x': 4, 'y': 0}, 'y': {'x': 5, 'y': 0},\n",
    "                                   'u': {'x': 6, 'y': 0}, 'i': {'x': 7, 'y': 0}, 'o': {'x': 8, 'y': 0},\n",
    "                                   'p': {'x': 9, 'y': 0}, 'a': {'x': 0, 'y': 1}, 'z': {'x': 0, 'y': 2},\n",
    "                                   's': {'x': 1, 'y': 1}, 'x': {'x': 1, 'y': 2}, 'd': {'x': 2, 'y': 1},\n",
    "                                   'c': {'x': 2, 'y': 2}, 'f': {'x': 3, 'y': 1}, 'b': {'x': 4, 'y': 2},\n",
    "                                   'm': {'x': 6, 'y': 2}, 'j': {'x': 6, 'y': 1}, 'g': {'x': 4, 'y': 1},\n",
    "                                   'h': {'x': 5, 'y': 1}, 'j': {'x': 6, 'y': 1}, 'k': {'x': 7, 'y': 1},\n",
    "                                   'l': {'x': 8, 'y': 1}, 'v': {'x': 3, 'y': 2}, 'n': {'x': 5, 'y': 2},\n",
    "                                   'ß': {'x': 10,'y': 2}, 'ü': {'x': 10,'y': 2}, 'ä': {'x': 10,'y': 0},\n",
    "                                   'ö': {'x': 11,'y': 0}}\n",
    "        self.nearest_to_i = self.get_nearest_to_i(self.keyboard_cartesian)\n",
    "        self.splits = self.stratified_split()\n",
    "\n",
    "\n",
    "    def get_nearest_to_i(self, keyboard_cartesian):\n",
    "        \"\"\" Get the nearest key to the one read.\n",
    "            @params: keyboard_cartesian The layout of the QWERTY keyboard for English\n",
    "            \n",
    "            return dictionary of eaculidean distances for the characters\"\"\"\n",
    "        nearest_to_i = {}\n",
    "        for i in keyboard_cartesian.keys():\n",
    "            nearest_to_i[i] = []\n",
    "            for j in keyboard_cartesian.keys():\n",
    "                if self._euclidean_distance(i, j) < mistake_distance: #was > 1.2\n",
    "                    nearest_to_i[i].append(j)\n",
    "        return nearest_to_i\n",
    "\n",
    "    def _shuffle_word(self, word, cutoff=0.7):\n",
    "        \"\"\" Rearange the given characters in a word simulating typos given a probability.\n",
    "        \n",
    "            @param: word A single word coming from a sentence\n",
    "            @param: cutoff The cutoff probability to make a change (default 0.9)\n",
    "            \n",
    "            return The word rearranged \n",
    "            \"\"\"\n",
    "        word = list(word.lower())\n",
    "        if random.uniform(0, 1.0) > cutoff:\n",
    "            loc = np.random.randint(0, len(word))\n",
    "            if word[loc] in self.keyboard_cartesian:\n",
    "                word[loc] = random.choice(self.nearest_to_i[word[loc]])\n",
    "        return ''.join(word)\n",
    "\n",
    "    def _euclidean_distance(self, a, b):\n",
    "        \"\"\" Calculates the euclidean between 2 points in the keyboard\n",
    "            @param: a Point one \n",
    "            @param: b Point two\n",
    "            \n",
    "            return The euclidean distance between the two points\"\"\"\n",
    "        X = (self.keyboard_cartesian[a]['x'] - self.keyboard_cartesian[b]['x']) ** 2\n",
    "        Y = (self.keyboard_cartesian[a]['y'] - self.keyboard_cartesian[b]['y']) ** 2\n",
    "        return math.sqrt(X + Y)\n",
    "\n",
    "    def _get_augment_sentence(self, sentence):\n",
    "        return ' '.join([self._shuffle_word(item) for item in sentence.split(' ')])\n",
    "    \n",
    "    def _augment_sentence(self, sentence, num_samples):\n",
    "        \"\"\" Augment the dataset of file with a sentence shuffled\n",
    "            @param: sentence The sentence from the set\n",
    "            @param: num_samples The number of sentences to genererate\n",
    "            \n",
    "            return A set of augmented sentences\"\"\"\n",
    "        sentences = []\n",
    "        for _ in range(num_samples):\n",
    "            sentences.append(self._get_augment_sentence(sentence))\n",
    "        sentences = list(set(sentences))\n",
    "        # print(\"sentences\", sentences)\n",
    "        return sentences + [sentence]\n",
    "\n",
    "    def _augment_split(self, X_train, y_train, num_samples=100):\n",
    "        \"\"\" Split the augmented train dataset\n",
    "            @param: X_train The full array of sentences\n",
    "            @param: y_train The train labels in the train dataset\n",
    "            @param: num_samples the number of new sentences to create (default 1000)\n",
    "            \n",
    "            return Augmented training dataset\"\"\"\n",
    "        Xs, ys = [], []\n",
    "        for X, y in zip(X_train, y_train):\n",
    "            tmp_x = self._augment_sentence(X, num_samples)\n",
    "            sample = [[Xs.append(item), ys.append(y)] for item in tmp_x]\n",
    "#             print(X, y)\n",
    "#             print(self.augmentedFile+str(self.nSamples)+\".csv\")\n",
    "            \n",
    "        with open(self.augmentedFile+str(self.nSamples)+\"Train.csv\", 'w', encoding='utf8') as csvFile:\n",
    "            fileWriter = csv.writer(csvFile, delimiter='\\t')\n",
    "            for i in range(0, len(Xs)-1):\n",
    "                fileWriter.writerow([Xs[i] + '\\t' + ys[i]])\n",
    "                # print(Xs[i], \"\\t\", ys[i])\n",
    "                # print(Xs[i])\n",
    "            # fileWriter.writerows(Xs + ['\\t'] + ys)\n",
    "        return Xs, ys\n",
    "\n",
    "    # Randomly replaces the nouns and verbs by synonyms\n",
    "    def _synonym_word(self, word, cutoff=0.5):\n",
    "        if random.uniform(0, 1.0) > cutoff and len(get_synonyms(word)) > 0 and word in nouns and word in verbs:\n",
    "            return random.choice(get_synonyms(word))\n",
    "        return word\n",
    "    \n",
    "    # Randomly replace words (nouns and verbs) in sentence by synonyms\n",
    "    def _get_synonym_sentence(self, sentence, cutoff = 0.5):\n",
    "        return ' '.join([self._synonym_word(item, cutoff) for item in sentence.split(' ')])\n",
    "\n",
    "    # For all classes except the largest ones; add duplicate (possibly augmented) samples until all classes have the same size\n",
    "    def _oversample_split(self, X_train, y_train, synonym_extra_samples = False, augment_extra_samples = False):\n",
    "        \"\"\" Split the oversampled train dataset\n",
    "            @param: X_train The full array of sentences\n",
    "            @param: y_train The train labels in the train dataset\n",
    "        \n",
    "            return Oversampled training dataset\"\"\"\n",
    "        \n",
    "        classes = {}\n",
    "        for X, y in zip(X_train, y_train):\n",
    "            if y not in classes:\n",
    "                classes[y] = []\n",
    "            classes[y].append(X)\n",
    "            \n",
    "        max_class_size = max([len(entries) for entries in classes.values()])\n",
    "        \n",
    "        Xs, ys = [],[] \n",
    "        for y in classes.keys():\n",
    "            for i in range(max_class_size):\n",
    "                sentence = classes[y][i % len(classes[y])]\n",
    "                if i >= len(classes[y]):\n",
    "                    if synonym_extra_samples:\n",
    "                        sentence = self._get_synonym_sentence(sentence)\n",
    "                    if augment_extra_samples:\n",
    "                        sentence = self._get_augment_sentence(sentence)\n",
    "                Xs.append(sentence)\n",
    "                ys.append(y)\n",
    "                \n",
    "        with open(\"./datasets/KL/Chatbot/train_augmented.csv\", 'w', encoding='utf8') as csvFile:\n",
    "            fileWriter = csv.writer(csvFile, delimiter='\\t')\n",
    "            for i in range(0, len(Xs)-1):\n",
    "                fileWriter.writerow([Xs[i] + '\\t' + ys[i]])\n",
    "               \n",
    "        return Xs, ys\n",
    "    \n",
    "    def _synonym_split(self, X_train, y_train, num_samples=100):\n",
    "        \"\"\" Split the augmented train dataset\n",
    "            @param: X_train The full array of sentences\n",
    "            @param: y_train The train labels in the train dataset\n",
    "            @param: num_samples the number of new sentences to create (default 1000)\n",
    "            \n",
    "            return Augmented training dataset\"\"\"\n",
    "        Xs, ys = [], []\n",
    "        for X, y in zip(X_train, y_train):\n",
    "            sample = [[Xs.append(self._get_synonym_sentence(X)), ys.append(y)] for item in range(additional_synonyms)]\n",
    "#             print(X, y)\n",
    "            \n",
    "        with open(\"./datasets/KL/Chatbot/train_augmented.csv\", 'w', encoding='utf8') as csvFile:\n",
    "            fileWriter = csv.writer(csvFile, delimiter='\\t')\n",
    "            for i in range(0, len(Xs)-1):\n",
    "                fileWriter.writerow([Xs[i] + '\\t' + ys[i]])\n",
    "        return Xs, ys\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" Load the file for now only the test.csv, train.csv files hardcoded\n",
    "        \n",
    "            return The vector separated in test, train and the labels for each one\"\"\"\n",
    "        with open(self.dataset_path) as csvfile:\n",
    "            readCSV = csv.reader(csvfile, delimiter='\t')\n",
    "            all_rows = list(readCSV)\n",
    "#             for i in all_rows:\n",
    "#                 if i ==  28823:\n",
    "#                     print(all_rows[i])\n",
    "            X_test = [a[0] for a in all_rows]\n",
    "            y_test = [a[1] for a in all_rows]\n",
    "\n",
    "        with open(self.dataset_path) as csvfile:\n",
    "            readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "            all_rows = list(readCSV)\n",
    "            X_train = [a[0] for a in all_rows]\n",
    "            y_train = [a[1] for a in all_rows]\n",
    "        return X_test, y_test, X_train, y_train\n",
    "\n",
    "    def process_sentence(self, x):\n",
    "        \"\"\" Clean the tokens from stop words in a sentence.\n",
    "            @param x Sentence to get rid of stop words.\n",
    "            \n",
    "            returns clean string sentence\"\"\"\n",
    "        clean_tokens = []\n",
    "        doc = nlp.tokenizer(x)\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                clean_tokens.append(token.lemma_)\n",
    "        return \" \".join(clean_tokens)\n",
    "\n",
    "    def process_batch(self, X):\n",
    "        \"\"\"See the progress as is coming along.\n",
    "        \n",
    "            return list[] of clean sentences\"\"\"\n",
    "        return [self.process_sentence(a) for a in tqdm(X)]\n",
    "\n",
    "    def stratified_split(self):\n",
    "        \"\"\" Split data whole into stratified test and training sets, then remove stop word from sentences\n",
    "        \n",
    "            return list of dictionaries with keys train,test and values the x and y for each one\"\"\"\n",
    "        self.X_train, self.X_test = ([preprocess(sentence) for sentence in self.X_train],[preprocess(sentence) for sentence in self.X_test])\n",
    "        print(self.X_train)\n",
    "        if oversample:\n",
    "            self.X_train, self.y_train = self._oversample_split(self.X_train, self.y_train, synonym_extra_samples, augment_extra_samples)\n",
    "        if additional_synonyms > 0:\n",
    "            self.X_train, self.y_train = self._synonym_split(self.X_train, self.y_train, additional_synonyms)\n",
    "        if additional_augments > 0:\n",
    "            self.X_train, self.y_train = self._augment_split(self.X_train, self.y_train, additional_augments)\n",
    "\n",
    "        splits = [{\"train\": {\"X\": self.X_train, \"y\": self.y_train},\n",
    "                   \"test\": {\"X\": self.X_test, \"y\": self.y_test}}]\n",
    "        return splits\n",
    "\n",
    "    def get_splits(self):\n",
    "        \"\"\" Get the splitted sentences\n",
    "            \n",
    "            return splitted list of dictionaries\"\"\"\n",
    "        return self.splits\n",
    "#****************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/KL/Chatbot/train.csv\n",
      "['what be the short connection between quiddestraãÿe and odeonsplatz ?', 'what be the cheap connection between quiddestraãÿe and hauptbahnhof ?', 'what be the short way between hauptbahnhof and odeonsplatz ?', 'how can i get from garch to mã¼nchner freiheit as fast as possible ?', 'what be the cheap way from neuperlach sã¼d to lehel ?', 'how can i get from neuperlach zentrum to karlsplatz as fast as possible ?', 'could -PRON- give -PRON- the fast connection between brudermã¼hlstraãÿe and alte heide ?', 'be there a train from neuperlach zentrum to garching at 3 pm ?', 'can -PRON- find a connection from olympiazentrum to lehel at 2 pm ?', 'i need a connection from harra to karl - preis - platz at 8 am .', 'in need to be at hauptbahnhof at 1 pm , can -PRON- search a connection from garch forschungszentrum ?', 'i need to be in garching at 9', 'can i take a bus from quiddestraãÿe to hauptbahnhof ?', 'can -PRON- find the short way from moosfeld to milbertshofen ?', 'how can i get to neuperlach sã¼d from garch forschungszentrum ?', 'can -PRON- find a bus from quiddestraãÿe to lehel ?', 'be there a tram from karlsplatz to lehel ?', 'be there a bus from garch to moosach at around 5 ?', 'be there a bus from odeonsplatz to hauptbahnhof at 3 pm ?', 'can -PRON- tell -PRON- the cheap way from garch forschungszentrum to quiddestraãÿe ?', 'how can i get to quiddestraãÿe ?', 'when do the next bus leave from hauptbahnhof ?', 'how can i get from garch to hauptbahnhof ?', 'how can i get from kurt - eisner - straãÿe to garching ?', 'when the next train in garching , forschungszentrum be leave ?', 'what be the next connection from garch forschungszentrum to odeonsplatz ?', 'how can i get from mossach to garch forschungszentrum ?', 'how can i get from garch forschungszentrum to odeonsplatz ?', 'how can i get from garching , forschungszentrum to odeonsplatz ?', 'how can i get from garch forschungszentrum to kurt - eisner - straãÿe ?', 'how can i get to boltzmannstraãÿe from quiddestraãÿe ?', 'when do the next train depart from quiddestraãÿe ?', 'when the next train in garching forschungszentrum be leave ?', 'when be the next u6 leave from garching ?', 'when do the next train leave from garch forschungszentrum', 'when be the next u6 ?', 'when be the next subway leave from garching ?', 'when the next train in garching be leave ?', 'when be the next train leave in garching', 'when do the next train leave from odeonsplatz ?', 'when do the next train leave from quiddestraãÿe ?', 'when be the next train', 'when do the next bus leave at garch forschungszentrum ?', 'when do the next train leaf ?', 'when do the next bus leave in garching ?', 'when do the next s - bahn leaf from hauptbahnhof ?', 'when do the next subway departe from odeonsplatz ?', 'show -PRON- the next bus from garching !', 'when do the next tram start from hauptbahnhof ?', 'when will the next u - bahn depart from garch forschungszentrum ?', 'show -PRON- the next bus from michaelibad .', 'when do the next train start at sendling tor ?', 'hey bot , when do the next bus start at garching ?', 'when do the next bus leave at garching ?', 'when be the bus from quiddestraãÿe ?', 'when can i get a bus at mariahilfplatz ?', 'when do the next bus leave at romanplatz ?', 'when do the bus to rã¶blingweg start ?', 'next bus from quiddestraãÿe ?', 'when be the next bus leave from garching ?', 'when be the train leave in garching', 'when be the train leave in garching ?', 'how can i get from garch to odeonsplatz ?', 'when be adrian next subway leave at garch forschungszentrum ?', 'how can i get from olympiazentrum to hauptbahnhof ?', 'how can i get from quiddestraãÿe to boltzmannstraãÿe ?', 'how can i get from moosach to garch forschungszentrum ?', 'can -PRON- give -PRON- a connection from garch to odeonsplatz ?', 'when be the next train from garching', 'when come the next train', 'when be the next train in nordfriedhof', 'when do the next train come at garching forschungszentrum', 'when be the next train from nordfriedhof to garch forschungszentrum', 'i want to travel from garch to odeonsplatz ?', 'how can i get to sendlinger tor ?', 'how do i get to untere straussã¤cker ?', 'how can i get from garch to garching ?', 'how can i get from garching to sendlinger tor ?', 'how can i get from garchingto garching ?', 'when be the next train from garching', 'how do i get from marienplatz zu garching', 'how can i get to milbertshofen from garching ?', 'when be the next train to garching', 'how can i get from garching to milbertshofen ?', 'when do the next rocket leave from garch forschungszentrum ?', 'connection from untere straãÿã¤cker 21 to kieferngarten', 'how to get from untere strassã¤cker 21 to frã¶tman', 'how to get from untere strassaecker 21 to frã¶tman', 'when be the next train from untere straãÿaecker 21 to kieferngarten', 'when be the next train from untere straãÿaecker 21 , garch to kieferngarten', 'from garch to perlach', 'how to get from garch to perlach ?', 'how to get from bonnerplatz to freimann', 'when be the next train in nordfriedhof ?', 'when be the next train in mã¼nchner freiheit ?', 'connection from hauptbahnhof to odeonsplatz ?', 'how do i get from olympia einkaufszentrum to hauptbahnhof ?', 'when be the next bus in garching forschungszentrum', 'how can i get from garching to marienplatz', 'from garch to studentenstadt']\n",
      "mera****************************\n",
      "['what be the short connection between quiddestraãÿe and odeonsplatz ?', 'what be the cheap connection between quiddestraãÿe and hauptbahnhof ?', 'what be the short way between hauptbahnhof and odeonsplatz ?', 'how can i get from garch to mã¼nchner freiheit as fast as possible ?', 'what be the cheap way from neuperlach sã¼d to lehel ?']\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "print(\"./datasets/KL/\" + benchmark_dataset + \"/train.csv\")\n",
    "dataset = MeraDataset(\"./datasets/KL/\" + benchmark_dataset + \"/train.csv\")\n",
    "print(\"mera****************************\")\n",
    "splits = dataset.get_splits()\n",
    "xS_train = []\n",
    "yS_train = []\n",
    "for elem in splits[0][\"train\"][\"X\"]:\n",
    "    xS_train.append(elem)\n",
    "print(xS_train[:5])\n",
    "\n",
    "for elem in splits[0][\"train\"][\"y\"]:\n",
    "    yS_train.append(intent_dict[elem])\n",
    "    \n",
    "print(len(xS_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "X_train_raw, y_train_raw = read_CSV_datafile(filename = filename_train)\n",
    "X_test_raw, y_test_raw = read_CSV_datafile(filename = filename_test)\n",
    "print(y_train_raw[:5])\n",
    "\n",
    "X_train_raw = xS_train\n",
    "y_train_raw = yS_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data samples: \n",
      " ['what be the short connection between quiddestraãÿe and odeonsplatz ?', 'what be the cheap connection between quiddestraãÿe and hauptbahnhof ?', 'what be the short way between hauptbahnhof and odeonsplatz ?', 'how can i get from garch to mã¼nchner freiheit as fast as possible ?', 'what be the cheap way from neuperlach sã¼d to lehel ?', 'how can i get from neuperlach zentrum to karlsplatz as fast as possible ?', 'could -PRON- give -PRON- the fast connection between brudermã¼hlstraãÿe and alte heide ?', 'be there a train from neuperlach zentrum to garching at 3 pm ?', 'can -PRON- find a connection from olympiazentrum to lehel at 2 pm ?', 'i need a connection from harra to karl - preis - platz at 8 am .', 'in need to be at hauptbahnhof at 1 pm , can -PRON- search a connection from garch forschungszentrum ?', 'i need to be in garching at 9', 'can i take a bus from quiddestraãÿe to hauptbahnhof ?', 'can -PRON- find the short way from moosfeld to milbertshofen ?', 'how can i get to neuperlach sã¼d from garch forschungszentrum ?', 'can -PRON- find a bus from quiddestraãÿe to lehel ?', 'be there a tram from karlsplatz to lehel ?', 'be there a bus from garch to moosach at around 5 ?', 'be there a bus from odeonsplatz to hauptbahnhof at 3 pm ?', 'can -PRON- tell -PRON- the cheap way from garch forschungszentrum to quiddestraãÿe ?', 'how can i get to quiddestraãÿe ?', 'how can i get from garch to hauptbahnhof ?', 'how can i get from kurt - eisner - straãÿe to garching ?', 'what be the next connection from garch forschungszentrum to odeonsplatz ?', 'how can i get from mossach to garch forschungszentrum ?', 'how can i get from garch forschungszentrum to odeonsplatz ?', 'how can i get from garching , forschungszentrum to odeonsplatz ?', 'how can i get from garch forschungszentrum to kurt - eisner - straãÿe ?', 'how can i get to boltzmannstraãÿe from quiddestraãÿe ?', 'how can i get from garch to odeonsplatz ?', 'how can i get from olympiazentrum to hauptbahnhof ?', 'how can i get from quiddestraãÿe to boltzmannstraãÿe ?', 'how can i get from moosach to garch forschungszentrum ?', 'can -PRON- give -PRON- a connection from garch to odeonsplatz ?', 'when be the next train from nordfriedhof to garch forschungszentrum', 'i want to travel from garch to odeonsplatz ?', 'how can i get to sendlinger tor ?', 'how do i get to untere straussã¤cker ?', 'how can i get from garch to garching ?', 'how can i get from garching to sendlinger tor ?', 'how can i get from garchingto garching ?', 'how do i get from marienplatz zu garching', 'how can i get to milbertshofen from garching ?', 'when be the next train to garching', 'how can i get from garching to milbertshofen ?', 'connection from untere straãÿã¤cker 21 to kieferngarten', 'how to get from untere strassã¤cker 21 to frã¶tman', 'how to get from untere strassaecker 21 to frã¶tman', 'when be the next train from untere straãÿaecker 21 to kieferngarten', 'when be the next train from untere straãÿaecker 21 , garch to kieferngarten', 'from garch to perlach', 'how to get from garch to perlach ?', 'how to get from bonnerplatz to freimann', 'connection from hauptbahnhof to odeonsplatz ?', 'how do i get from olympia einkaufszentrum to hauptbahnhof ?', 'how can i get from garching to marienplatz', 'from garch to studentenstadt', 'when do the next bus leave from hauptbahnhof ?', 'when the next train in garching , forschungszentrum be leave ?', 'when do the next train depart from quiddestraãÿe ?', 'when the next train in garching forschungszentrum be leave ?', 'when be the next u6 leave from garching ?', 'when do the next train leave from garch forschungszentrum', 'when be the next u6 ?', 'when be the next subway leave from garching ?', 'when the next train in garching be leave ?', 'when be the next train leave in garching', 'when do the next train leave from odeonsplatz ?', 'when do the next train leave from quiddestraãÿe ?', 'when be the next train', 'when do the next bus leave at garch forschungszentrum ?', 'when do the next train leaf ?', 'when do the next bus leave in garching ?', 'when do the next s - bahn leaf from hauptbahnhof ?', 'when do the next subway departe from odeonsplatz ?', 'show -PRON- the next bus from garching !', 'when do the next tram start from hauptbahnhof ?', 'when will the next u - bahn depart from garch forschungszentrum ?', 'show -PRON- the next bus from michaelibad .', 'when do the next train start at sendling tor ?', 'hey bot , when do the next bus start at garching ?', 'when do the next bus leave at garching ?', 'when be the bus from quiddestraãÿe ?', 'when can i get a bus at mariahilfplatz ?', 'when do the next bus leave at romanplatz ?', 'when do the bus to rã¶blingweg start ?', 'next bus from quiddestraãÿe ?', 'when be the next bus leave from garching ?', 'when be the train leave in garching', 'when be the train leave in garching ?', 'when be adrian next subway leave at garch forschungszentrum ?', 'when be the next train from garching', 'when come the next train', 'when be the next train in nordfriedhof', 'when do the next train come at garching forschungszentrum', 'when be the next train from garching', 'when do the next rocket leave from garch forschungszentrum ?', 'when be the next train in nordfriedhof ?', 'when be the next train in mã¼nchner freiheit ?', 'when be the next bus in garching forschungszentrum', 'dhen do the next bue leave from hauptbahnhof ?', 'whcn ehe nexe train in garching , forschuygszentrum be leave ?', 'when so the next train depare from quiddestraãÿe ?', 'qhen the nexh train in garching forschungfzentrum be leave ?', 'when be the next u6 leave from garching ?', 'wyen do the next train leave from garch forschungszentrum', 'when be tue next u6 ?', 'when be the next subway leave from garzhing ?', 'when the next traib ij garching ne leave ?', 'when te the next train leave yn garching', 'when do the next train leave from odeonsplatz ?', 'when do the next train leave from quisdestraãÿe ?', 'when be the next traij', 'when du rhe next bus leave wt garch forschungszentrum ?'] \n",
      "\n",
      "\n",
      "Class Labels: \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "Size of Training Data: 114\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data samples: \\n\",X_train_raw, \"\\n\\n\")\n",
    "\n",
    "print(\"Class Labels: \\n\", y_train_raw, \"\\n\\n\")\n",
    "\n",
    "print(\"Size of Training Data: {}\".format(len(X_train_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "def semhash_tokenizer(text):\n",
    "    tokens = text.split(\" \")\n",
    "    final_tokens = []\n",
    "    for unhashed_token in tokens:\n",
    "        hashed_token = \"#{}#\".format(unhashed_token)\n",
    "        final_tokens += [''.join(gram)\n",
    "                         for gram in list(find_ngrams(list(hashed_token), 3))]\n",
    "    return final_tokens\n",
    "\n",
    "def semhash_corpus(corpus):\n",
    "    new_corpus = []\n",
    "    for sentence in corpus:\n",
    "        sentence = preprocess(sentence)\n",
    "        tokens = semhash_tokenizer(sentence)\n",
    "        new_corpus.append(\" \".join(map(str,tokens)))\n",
    "    return new_corpus\n",
    "\n",
    "X_train_raw = semhash_corpus(X_train_raw)\n",
    "X_test_raw = semhash_corpus(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#wh wha hat at# #be be# #th the he# #sh sho hor ort rt# #co con onn nne nec ect cti tio ion on# #be bet etw twe wee een en# #qu qui uid idd dde des est str tra raã aãÿ ãÿe ÿe# #an and nd# #od ode deo eon ons nsp spl pla lat atz tz# #?#', '#wh wha hat at# #be be# #th the he# #ch che hea eap ap# #co con onn nne nec ect cti tio ion on# #be bet etw twe wee een en# #qu qui uid idd dde des est str tra raã aãÿ ãÿe ÿe# #an and nd# #ha hau aup upt ptb tba bah ahn hnh nho hof of# #?#', '#wh wha hat at# #be be# #th the he# #sh sho hor ort rt# #wa way ay# #be bet etw twe wee een en# #ha hau aup upt ptb tba bah ahn hnh nho hof of# #an and nd# #od ode deo eon ons nsp spl pla lat atz tz# #?#', '#ho how ow# #ca can an# #i# #ge get et# #fr fro rom om# #ga gar arc rch ch# #to to# #mã mã¼ ã¼n ¼nc nch chn hne ner er# #fr fre rei eih ihe hei eit it# #as as# #fa fas ast st# #as as# #po pos oss ssi sib ibl ble le# #?#', '#wh wha hat at# #be be# #th the he# #ch che hea eap ap# #wa way ay# #fr fro rom om# #ne neu eup upe per erl rla lac ach ch# #sã sã¼ ã¼d ¼d# #to to# #le leh ehe hel el# #?#']\n",
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw[:5])\n",
    "print(y_train_raw[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(corpus, preprocessor=None, tokenizer=None):\n",
    "    vectorizer = CountVectorizer(ngram_range=(2,4),analyzer='char')\n",
    "    vectorizer.fit(corpus)\n",
    "    return vectorizer, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n",
    "              print_report=True, feature_names=None, print_top10=False,\n",
    "              print_cm=True):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    f1_score = metrics.f1_score(y_test, pred, average='weighted')\n",
    "    \n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    #print(\"Accuracy: %0.3f (+/- %0.3f)\" % (score.mean(), score.std() * 2))\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate([\"Make Update\", \"Setup Printer\", \"Shutdown Computer\",\"Software Recommendation\", \"None\"]):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join([feature_names[i] for i in top10]))))\n",
    "        print()\n",
    "\n",
    "    if print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,labels = range(len(target_names)),\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "    \n",
    "    with open(\"./results.csv\", 'a', encoding='utf8') as csvFile:\n",
    "        fileWriter = csv.writer(csvFile, delimiter='\\t')\n",
    "        fileWriter.writerow([benchmark_dataset,str(clf),str(oversample),str(synonym_extra_samples),str(augment_extra_samples),\n",
    "                             str(additional_synonyms),str(additional_augments), str(mistake_distance), str(score), str(f1_score)])\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    # make some plots\n",
    "    indices = np.arange(len(results))\n",
    "\n",
    "    results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "    clf_names, score, training_time, test_time = results\n",
    "    training_time = np.array(training_time) / np.max(training_time)\n",
    "    test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Score\")\n",
    "    plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "    plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "             color='c')\n",
    "    plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "    plt.yticks(())\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplots_adjust(left=.25)\n",
    "    plt.subplots_adjust(top=.95)\n",
    "    plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "    for i, c in zip(indices, clf_names):\n",
    "        plt.text(-.3, i, c)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_training():\n",
    "    vectorizer, feature_names = get_vectorizer(X_train_raw, preprocessor=preprocess, tokenizer=tokenize)\n",
    "    \n",
    "    X_train_no_HD = vectorizer.transform(X_train_raw).toarray()\n",
    "    X_test_no_HD = vectorizer.transform(X_test_raw).toarray()\n",
    "            \n",
    "    return X_train_no_HD, y_train_raw, X_test_no_HD, y_test_raw, feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     X_train, y_train, X_test, y_test, feature_names = data_for_training()\n",
    "def ngram_encode(str_test, HD_aphabet, aphabet, n_size): # method for mapping n-gram statistics of a word to an N-dimensional HD vector\n",
    "    HD_ngram = np.zeros(HD_aphabet.shape[1]) # will store n-gram statistics mapped to HD vector\n",
    "    full_str = '#' + str_test + '#' # include extra symbols to the string\n",
    "        \n",
    "    for il, l in enumerate(full_str[:-(n_size-1)]): # loops through all n-grams\n",
    "        hdgram = HD_aphabet[aphabet.find(full_str[il]), :] # picks HD vector for the first symbol in the current n-gram\n",
    "        for ng in range(1, n_size): #loops through the rest of symbols in the current n-gram\n",
    "            hdgram = hdgram * np.roll(HD_aphabet[aphabet.find(full_str[il+ng]), :], ng) # two operations simultaneously; binding via elementvise multiplication; rotation via cyclic shift\n",
    "            \n",
    "        HD_ngram += hdgram # increments HD vector of n-gram statistics with the HD vector for the currently observed n-gram\n",
    "    \n",
    "    HD_ngram_norm = np.sqrt(HD_aphabet.shape[1]) * (HD_ngram/ np.linalg.norm(HD_ngram) )  # normalizes HD-vector so that its norm equals sqrt(N)       \n",
    "    return HD_ngram_norm # output normalized HD mapping\n",
    "\n",
    "\n",
    "\n",
    "N = 1000 # set the desired dimensionality of HD vectors\n",
    "n_size=3 # n-gram size\n",
    "aphabet = 'abcdefghijklmnopqrstuvwxyz#' #fix the alphabet. Note, we assume that capital letters are not in use \n",
    "np.random.seed(1) # for reproducibility\n",
    "HD_aphabet = 2 * (np.random.randn(len(aphabet), N) < 0) - 1 # generates bipolar {-1, +1}^N HD vectors; one random HD vector per symbol in the alphabet\n",
    "\n",
    "# str='High like a basketball jump' # example string to represent using n-grams\n",
    "\n",
    "# print(len(ngram_encode(str, HD_aphabet, aphabet, n_size))) # HD_ngram is a projection of n-gram statistics for str to N-dimensional space. It can be used to learn the word embedding\n",
    "print(X_train_raw[:1])\n",
    "\n",
    "for i in range(len(X_train_raw)):\n",
    "     X_train_raw[i] = ngram_encode(X_train_raw[i], HD_aphabet, aphabet, n_size) # HD_ngram is a projection of n-gram statistics for str to N-dimensional space. It can be used to learn the word embedding\n",
    "print(X_train_raw[:5])\n",
    "for i in range(len(X_test_raw)):\n",
    "    X_test_raw[i] = ngram_encode(X_test_raw[i], HD_aphabet, aphabet, n_size)\n",
    "print(X_test_raw[:5])\n",
    "\n",
    "X_train, y_train, X_test, y_test = X_train_raw, y_train_raw, X_test_raw, y_test_raw\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Split 0\n",
      "Train Size: 114\n",
      "Test Size: 106\n",
      "================================================================================\n",
      "gridsearchRF\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
      "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10000900,\n",
      "            n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'n_estimators': [50, 60, 70], 'min_samples_leaf': [1, 11]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusgru\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.386s\n",
      "test time:  0.004s\n",
      "accuracy:   0.981\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       1.00      0.94      0.97        35\n",
      "Find Connection       0.97      1.00      0.99        71\n",
      "\n",
      "      micro avg       0.98      0.98      0.98       106\n",
      "      macro avg       0.99      0.97      0.98       106\n",
      "   weighted avg       0.98      0.98      0.98       106\n",
      "\n",
      "confusion matrix:\n",
      "[[33  2]\n",
      " [ 0 71]]\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "train time: 0.006s\n",
      "test time:  0.002s\n",
      "accuracy:   0.981\n",
      "dimensionality: 3284\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       1.00      0.94      0.97        35\n",
      "Find Connection       0.97      1.00      0.99        71\n",
      "\n",
      "      micro avg       0.98      0.98      0.98       106\n",
      "      macro avg       0.99      0.97      0.98       106\n",
      "   weighted avg       0.98      0.98      0.98       106\n",
      "\n",
      "confusion matrix:\n",
      "[[33  2]\n",
      " [ 0 71]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=50, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusgru\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.030s\n",
      "test time:  0.002s\n",
      "accuracy:   0.943\n",
      "dimensionality: 3284\n",
      "density: 0.587393\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       1.00      0.83      0.91        35\n",
      "Find Connection       0.92      1.00      0.96        71\n",
      "\n",
      "      micro avg       0.94      0.94      0.94       106\n",
      "      macro avg       0.96      0.91      0.93       106\n",
      "   weighted avg       0.95      0.94      0.94       106\n",
      "\n",
      "confusion matrix:\n",
      "[[29  6]\n",
      " [ 0 71]]\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "train time: 0.015s\n",
      "test time:  0.002s\n",
      "accuracy:   0.981\n",
      "dimensionality: 3284\n",
      "density: 0.007004\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       0.97      0.97      0.97        35\n",
      "Find Connection       0.99      0.99      0.99        71\n",
      "\n",
      "      micro avg       0.98      0.98      0.98       106\n",
      "      macro avg       0.98      0.98      0.98       106\n",
      "   weighted avg       0.98      0.98      0.98       106\n",
      "\n",
      "confusion matrix:\n",
      "[[34  1]\n",
      " [ 1 70]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=50, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusgru\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.079s\n",
      "test time:  0.002s\n",
      "accuracy:   0.953\n",
      "dimensionality: 3284\n",
      "density: 0.593179\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       1.00      0.86      0.92        35\n",
      "Find Connection       0.93      1.00      0.97        71\n",
      "\n",
      "      micro avg       0.95      0.95      0.95       106\n",
      "      macro avg       0.97      0.93      0.94       106\n",
      "   weighted avg       0.96      0.95      0.95       106\n",
      "\n",
      "confusion matrix:\n",
      "[[30  5]\n",
      " [ 0 71]]\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=50, n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusgru\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.091s\n",
      "test time:  0.002s\n",
      "accuracy:   0.962\n",
      "dimensionality: 3284\n",
      "density: 0.561206\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       1.00      0.89      0.94        35\n",
      "Find Connection       0.95      1.00      0.97        71\n",
      "\n",
      "      micro avg       0.96      0.96      0.96       106\n",
      "      macro avg       0.97      0.94      0.96       106\n",
      "   weighted avg       0.96      0.96      0.96       106\n",
      "\n",
      "confusion matrix:\n",
      "[[31  4]\n",
      " [ 0 71]]\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.004s\n",
      "test time:  0.004s\n",
      "accuracy:   0.934\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       0.85      0.97      0.91        35\n",
      "Find Connection       0.98      0.92      0.95        71\n",
      "\n",
      "      micro avg       0.93      0.93      0.93       106\n",
      "      macro avg       0.92      0.94      0.93       106\n",
      "   weighted avg       0.94      0.93      0.93       106\n",
      "\n",
      "confusion matrix:\n",
      "[[34  1]\n",
      " [ 6 65]]\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.008s\n",
      "test time:  0.002s\n",
      "accuracy:   0.981\n",
      "dimensionality: 3284\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       0.97      0.97      0.97        35\n",
      "Find Connection       0.99      0.99      0.99        71\n",
      "\n",
      "      micro avg       0.98      0.98      0.98       106\n",
      "      macro avg       0.98      0.98      0.98       106\n",
      "   weighted avg       0.98      0.98      0.98       106\n",
      "\n",
      "confusion matrix:\n",
      "[[34  1]\n",
      " [ 1 70]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.008s\n",
      "test time:  0.008s\n",
      "accuracy:   0.991\n",
      "dimensionality: 3284\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       1.00      0.97      0.99        35\n",
      "Find Connection       0.99      1.00      0.99        71\n",
      "\n",
      "      micro avg       0.99      0.99      0.99       106\n",
      "      macro avg       0.99      0.99      0.99       106\n",
      "   weighted avg       0.99      0.99      0.99       106\n",
      "\n",
      "confusion matrix:\n",
      "[[34  1]\n",
      " [ 0 71]]\n",
      "\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0),\n",
      "        max_features=None, no...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusgru\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.018s\n",
      "test time:  0.001s\n",
      "accuracy:   0.972\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       0.97      0.94      0.96        35\n",
      "Find Connection       0.97      0.99      0.98        71\n",
      "\n",
      "      micro avg       0.97      0.97      0.97       106\n",
      "      macro avg       0.97      0.96      0.97       106\n",
      "   weighted avg       0.97      0.97      0.97       106\n",
      "\n",
      "confusion matrix:\n",
      "[[33  2]\n",
      " [ 1 70]]\n",
      "\n",
      "================================================================================\n",
      "KMeans\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=0, tol=0.0001, verbose=0)\n",
      "train time: 0.161s\n",
      "test time:  0.005s\n",
      "accuracy:   0.075\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       0.03      0.06      0.04        35\n",
      "Find Connection       0.15      0.08      0.11        71\n",
      "\n",
      "      micro avg       0.08      0.08      0.08       106\n",
      "      macro avg       0.09      0.07      0.07       106\n",
      "   weighted avg       0.11      0.08      0.09       106\n",
      "\n",
      "confusion matrix:\n",
      "[[ 2 33]\n",
      " [65  6]]\n",
      "\n",
      "================================================================================\n",
      "LogisticRegression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train time: 0.011s\n",
      "test time:  0.001s\n",
      "accuracy:   0.991\n",
      "dimensionality: 3284\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Departure Time       1.00      0.97      0.99        35\n",
      "Find Connection       0.99      1.00      0.99        71\n",
      "\n",
      "      micro avg       0.99      0.99      0.99       106\n",
      "      macro avg       0.99      0.99      0.99       106\n",
      "   weighted avg       0.99      0.99      0.99       106\n",
      "\n",
      "confusion matrix:\n",
      "[[34  1]\n",
      " [ 0 71]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for _ in enumerate(range(1)):\n",
    "    i_s = 0\n",
    "    split = 0\n",
    "    print(\"Evaluating Split {}\".format(i_s))\n",
    "    X_train, y_train, X_test, y_test, feature_names = data_for_training()\n",
    "    target_names = None\n",
    "    if benchmark_dataset == \"Chatbot\":\n",
    "        target_names = [\"Departure Time\", \"Find Connection\"]\n",
    "    elif benchmark_dataset == \"AskUbuntu\":\n",
    "        target_names = [\"Make Update\", \"Setup Printer\", \"Shutdown Computer\",\"Software Recommendation\", \"None\"]\n",
    "    elif benchmark_dataset == \"WebApplication\":\n",
    "        target_names = [\"Download Video\", \"Change Password\", \"None\", \"Export Data\", \"Sync Accounts\",\n",
    "                  \"Filter Spam\", \"Find Alternative\", \"Delete Account\"]\n",
    "    print(\"Train Size: {}\\nTest Size: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "    results = []\n",
    "    #alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "    parameters_mlp={'hidden_layer_sizes':[(100,50), (300, 100),(300,200,100)]}\n",
    "    parameters_RF={ \"n_estimators\" : [50,60,70],\n",
    "           \"min_samples_leaf\" : [1, 11]}\n",
    "    k_range = list(range(3,7))\n",
    "    parameters_knn = {'n_neighbors':k_range}\n",
    "    knn=KNeighborsClassifier(n_neighbors=5)\n",
    "    for clf, name in [  \n",
    "            #(RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "            #(GridSearchCV(knn,parameters_knn, cv=5),\"gridsearchknn\"),\n",
    "            #(Perceptron(n_iter=50), \"Perceptron\"),\n",
    "            #(GridSearchCV(MLPClassifier(activation='tanh'),parameters_mlp, cv=5),\"gridsearchmlp\"),\n",
    "           # (MLPClassifier(hidden_layer_sizes=(100, 50), activation=\"logistic\", max_iter=300), \"MLP\"),\n",
    "            #(MLPClassifier(hidden_layer_sizes=(300, 100, 50), activation=\"logistic\", max_iter=500), \"MLP\"),\n",
    "           # (MLPClassifier(hidden_layer_sizes=(300, 100, 50), activation=\"tanh\", max_iter=500), \"MLP\"),\n",
    "            #(PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "           # (KNeighborsClassifier(n_neighbors=1), \"kNN\"),\n",
    "           # (KNeighborsClassifier(n_neighbors=3), \"kNN\"),\n",
    "           # (KNeighborsClassifier(n_neighbors=5), \"kNN\"),\n",
    "            #(KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "            (GridSearchCV(RandomForestClassifier(n_estimators=10000900),parameters_RF, cv=5),\"gridsearchRF\")\n",
    "            #(RandomForestClassifier(n_estimators=10), \"Random forest\"),\n",
    "            #(RandomForestClassifier(n_estimators=50), \"Random forest\")\n",
    "    ]:\n",
    "           \n",
    "        print('=' * 80)\n",
    "        print(name)\n",
    "        result = benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n",
    "                                 feature_names=feature_names)\n",
    "        results.append(result)\n",
    "        \n",
    "       # print('parameters')\n",
    "       # print(clf.grid_scores_[0])\n",
    "        #print('CV Validation Score')\n",
    "       # print(clf.grid_scores_[0].cv_validation_scores)\n",
    "       # print('Mean Validation Score')\n",
    "       # print(clf.grid_scores_[0].mean_validation_score)\n",
    "       # grid_mean_scores = [result.mean_validation_score for result in clf.grid_scores_]\n",
    "       # print(grid_mean_scores)\n",
    "       # plt.plot(k_range, grid_mean_scores)\n",
    "       # plt.xlabel('Value of K for KNN')\n",
    "       # plt.ylabel('Cross-Validated Accuracy')\n",
    "\n",
    "    #parameters_Linearsvc = [{'C': [1, 10], 'gamma': [0.1,1.0]}]\n",
    "    for penalty in [\"l2\", \"l1\"]:\n",
    "        print('=' * 80)\n",
    "        print(\"%s penalty\" % penalty.upper())\n",
    "        # Train Liblinear model\n",
    "        #grid=(GridSearchCV(LinearSVC,parameters_Linearsvc, cv=10),\"gridsearchSVC\")\n",
    "        #results.append(benchmark(LinearSVC(penalty=penalty), X_train, y_train, X_test, y_test, target_names,\n",
    "                                # feature_names=feature_names))\n",
    "        \n",
    "        result = benchmark(LinearSVC(penalty=penalty, dual=False,tol=1e-3),\n",
    "                                 X_train, y_train, X_test, y_test, target_names,\n",
    "                                 feature_names=feature_names)\n",
    "        results.append(result)\n",
    "\n",
    "        # Train SGD model\n",
    "        result = benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                               penalty=penalty),\n",
    "                                 X_train, y_train, X_test, y_test, target_names,\n",
    "                                 feature_names=feature_names)\n",
    "        results.append(result)\n",
    "\n",
    "    # Train SGD with Elastic Net penalty\n",
    "    print('=' * 80)\n",
    "    print(\"Elastic-Net penalty\")\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=\"elasticnet\"),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "\n",
    "    # Train NearestCentroid without threshold\n",
    "    print('=' * 80)\n",
    "    print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "    results.append(benchmark(NearestCentroid(),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "\n",
    "    # Train sparse Naive Bayes classifiers\n",
    "    print('=' * 80)\n",
    "    print(\"Naive Bayes\")\n",
    "    results.append(benchmark(MultinomialNB(alpha=.01),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "    \n",
    "    result = benchmark(BernoulliNB(alpha=.01),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names)\n",
    "    results.append(result)\n",
    "\n",
    "    print('=' * 80)\n",
    "    print(\"LinearSVC with L1-based feature selection\")\n",
    "    # The smaller C, the stronger the regularization.\n",
    "    # The more regularization, the more sparsity.\n",
    "    \n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "    result = benchmark(Pipeline([\n",
    "                                  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                                                  tol=1e-3))),\n",
    "                                  ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names)\n",
    "    results.append(result)\n",
    "   # print(grid.grid_scores_)\n",
    "   #KMeans clustering algorithm \n",
    "    print('=' * 80)\n",
    "    print(\"KMeans\")\n",
    "    results.append(benchmark(KMeans(n_clusters=2, init='k-means++', max_iter=300,\n",
    "                verbose=0, random_state=0, tol=1e-4),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "    \n",
    "   \n",
    "    \n",
    "    print('=' * 80)\n",
    "    print(\"LogisticRegression\")\n",
    "    #kfold = model_selection.KFold(n_splits=2, random_state=0)\n",
    "    #model = LinearDiscriminantAnalysis()\n",
    "    results.append(benchmark(LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "    \n",
    "    plot_results(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
